{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqg4WPlcSvdiOFfroKPLYf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jayzilva/API-asp.net-core-controllers/blob/main/multi_model_comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UZBDDXjfwDV",
        "outputId": "cf238dec-fb6d-4abc-b266-9430c64e1968"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.10/dist-packages (0.11.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq) (0.27.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.23.4)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: python-pptx in /usr/local/lib/python3.10/dist-packages (1.0.2)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from python-pptx) (10.4.0)\n",
            "Requirement already satisfied: XlsxWriter>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from python-pptx) (3.2.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-pptx) (5.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-pptx) (4.12.2)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (2.9.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic) (4.12.2)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.10/dist-packages (1.24.13)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.5.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.4-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.4.2 (from gradio)\n",
            "  Downloading gradio_client-1.4.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Collecting huggingface-hub>=0.25.1 (from gradio)\n",
            "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.11)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart==0.0.12 (from gradio)\n",
            "  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.7.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<1.0,>=0.1.1 (from gradio)\n",
            "  Downloading safehttpx-0.1.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.41.2-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.2->gradio) (2024.10.0)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.4.2->gradio)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.66.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.5.0-py3-none-any.whl (56.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.7/56.7 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.4.2-py3-none-any.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.8/319.8 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n",
            "Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.4-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading ruff-0.7.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.1-py3-none-any.whl (8.4 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.41.2-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, websockets, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, huggingface-hub, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.24.7\n",
            "    Uninstalling huggingface-hub-0.24.7:\n",
            "      Successfully uninstalled huggingface-hub-0.24.7\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.4 ffmpy-0.4.0 gradio-5.5.0 gradio-client-1.4.2 huggingface-hub-0.26.2 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.12 ruff-0.7.3 safehttpx-0.1.1 semantic-version-2.10.0 starlette-0.41.2 tomlkit-0.12.0 uvicorn-0.32.0 websockets-12.0\n"
          ]
        }
      ],
      "source": [
        "! pip install groq\n",
        "! pip install python-dotenv\n",
        "! pip install python-pptx\n",
        "! pip install pydantic\n",
        "! pip install pymupdf\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "from dotenv import load_dotenv\n",
        "from groq import Groq\n",
        "from pptx import Presentation\n",
        "from pydantic import BaseModel, Field, validator\n",
        "from typing import Optional, Union, Dict, Any\n",
        "import gradio as gr\n",
        "import fitz\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Define the BusinessTermSheet model using Pydantic\n",
        "class BusinessTermSheet(BaseModel):\n",
        "    asset_class: Optional[str] = Field(None, alias=\"Asset Class\")\n",
        "    website: Optional[str] = Field(None, alias=\"Website\")\n",
        "    primary_impact: Optional[str] = Field(None, alias=\"Primary Impact\")\n",
        "    un_sustainable_development_goal: Optional[str] = Field(None, alias=\"UN Sustainable Development Goal\")\n",
        "    current_fund: Optional[str] = Field(None, alias=\"Current Fund\")\n",
        "    important_dates: Optional[Union[str, list]] = Field(None, alias=\"Important Dates\")\n",
        "    target_irr: Optional[str] = Field(None, alias=\"Target IRR\")\n",
        "    term: Optional[str] = Field(None, alias=\"Term\")\n",
        "    fund_domicile: Optional[str] = Field(None, alias=\"Fund Domicile\")\n",
        "    target_fund_size: Optional[str] = Field(None, alias=\"Target Fund Size\")\n",
        "    firm_strategy_overview: Optional[str] = Field(None, alias=\"Firm & Strategy Overview\")\n",
        "    total_firm_aum: Optional[str] = Field(None, alias=\"Total Firm AUM\")\n",
        "    strategy_differentiator: Optional[Union[str, Dict[str, Any]]] = Field(None, alias=\"Strategy Differentiator\")\n",
        "    key_financial_data: Optional[Union[str, Dict[str, Any]]] = Field(None, alias=\"Key Financial Data\")\n",
        "    investment_team_info: Optional[Union[str, Dict[str, Any]]] = Field(None, alias=\"Investment Team Size & Information\")\n",
        "    contact_details: Optional[Union[str, Dict[str, Any]]] = Field(None, alias=\"Contact Details\")\n",
        "\n",
        "    # Validator to ensure certain fields are always returned as strings\n",
        "    @validator(\"important_dates\", \"investment_team_info\", pre=True, always=True)\n",
        "    def ensure_string(cls, v):\n",
        "        if isinstance(v, list):\n",
        "            return \", \".join(v)\n",
        "        elif isinstance(v, dict):\n",
        "            return \", \".join([f\"{key}: {value}\" for key, value in v.items()])\n",
        "        return v\n",
        "\n",
        "# Extract text from PowerPoint (.pptx) files\n",
        "def extract_text_from_pptx(pptx_path):\n",
        "    presentation = Presentation(pptx_path)\n",
        "    text = \"\"\n",
        "    for slide in presentation.slides:\n",
        "        for shape in slide.shapes:\n",
        "            if hasattr(shape, \"text\"):\n",
        "                text += shape.text + \"\\n\"\n",
        "    return text\n",
        "\n",
        "# Extract text from PDF files\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with fitz.open(pdf_path) as doc:\n",
        "        for page in doc:\n",
        "            text += page.get_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "# Parse the raw text response into a dictionary\n",
        "def parse_response_to_dict(response_text):\n",
        "    data = {}\n",
        "    lines = response_text.splitlines()\n",
        "    for line in lines:\n",
        "        if \": \" in line:\n",
        "            key, value = line.split(\": \", 1)\n",
        "            data[key.strip()] = value.strip() or None\n",
        "    return data\n",
        "\n",
        "# Analyze the presentation file, extract details, and return results in JSON format\n",
        "def analyze_presentation(file_path, file_type, model_name):\n",
        "    # Extract text based on file type\n",
        "    if file_type == \"pptx\":\n",
        "        text_content = extract_text_from_pptx(file_path)\n",
        "    elif file_type == \"pdf\":\n",
        "        text_content = extract_text_from_pdf(file_path)\n",
        "    else:\n",
        "        return {\"Error\": \"Unsupported file type\"}\n",
        "\n",
        "    # Initialize Groq client and make API call\n",
        "    client = Groq(api_key=('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'))\n",
        "    stream = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are the reader of the presentation and analyzer.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"\"\"Extract only the following details from the presentation content related to a business term sheet:\n",
        "                - Asset Class\n",
        "                - Website\n",
        "                - Primary Impact\n",
        "                - UN Sustainable Development Goal\n",
        "                - Current Fund\n",
        "                - Important Dates\n",
        "                - Target IRR\n",
        "                - Term\n",
        "                - Fund Domicile\n",
        "                - Target Fund Size\n",
        "                - Firm & Strategy Overview\n",
        "                - Total Firm AUM\n",
        "                - Strategy Differentiator (Provide information on: Team, deal sourcing, due diligence, underwriting experience, execution, servicing)\n",
        "                - Key Financial Data (prior Track Record from other funds or prior fund)\n",
        "                - Investment Team Size & Information\n",
        "                - Contact Details\n",
        "\n",
        "                Here is the presentation content. Return the result only in JSON format: {text_content}\"\"\"\n",
        "            }\n",
        "        ],\n",
        "        model=model_name,\n",
        "        temperature=0.2,\n",
        "        max_tokens=1000,\n",
        "        top_p=0.5,\n",
        "        frequency_penalty=0.1,\n",
        "        presence_penalty=0.1,\n",
        "        stop=None,\n",
        "        stream=True,\n",
        "    )\n",
        "\n",
        "    full_response = \"\"\n",
        "    try:\n",
        "        for chunk in stream:\n",
        "            if chunk.choices and chunk.choices[0].delta:\n",
        "                content = chunk.choices[0].delta.content\n",
        "                if content is not None:\n",
        "                    full_response += content\n",
        "\n",
        "        match = re.search(r\"\\{.*\\}\", full_response.strip(), re.DOTALL)\n",
        "        if match:\n",
        "            json_response = json.loads(match.group(0))\n",
        "            structured_data = BusinessTermSheet(**json_response)\n",
        "            full_data = structured_data.dict()\n",
        "\n",
        "            # Filter out keys with null values\n",
        "            filtered_data = {k: v for k, v in full_data.items() if v is not None}\n",
        "            return filtered_data\n",
        "        else:\n",
        "            # Fallback to plain text parsing if JSON not found\n",
        "            fallback_data = parse_response_to_dict(full_response)\n",
        "            structured_data = BusinessTermSheet(**fallback_data)\n",
        "            full_data = structured_data.dict()\n",
        "\n",
        "            # Filter out keys with null values\n",
        "            filtered_data = {k: v for k, v in full_data.items() if v is not None}\n",
        "            return filtered_data\n",
        "    except json.JSONDecodeError:\n",
        "        # Handle JSON decode error with fallback parsing\n",
        "        fallback_data = parse_response_to_dict(full_response)\n",
        "        filtered_fallback_data = {k: v for k, v in fallback_data.items() if v is not None}\n",
        "        return {\"Error\": \"JSON data not found in response; used plain text parsing\", \"Data\": filtered_fallback_data}\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred while analyzing presentation: {e}\")\n",
        "        return {\"Error\": str(e), \"Full Response\": full_response}\n",
        "\n",
        "# Compare outputs from two models based on similarity\n",
        "def compare_outputs(output1, output2):\n",
        "    matching_fields = sum(1 for k in output1 if k in output2 and output1[k] == output2[k])\n",
        "    total_fields = max(len(output1), len(output2))\n",
        "    similarity_score = (matching_fields / total_fields) * 100 if total_fields > 0 else 0\n",
        "    return similarity_score\n",
        "\n",
        "# Process multiple presentations and compare outputs from two models\n",
        "def process_presentations(files, model_name1, model_name2):\n",
        "    combined_results = {}\n",
        "    for file in files:\n",
        "        file_type = file.name.split('.')[-1].lower()\n",
        "        if file_type in [\"pptx\", \"pdf\"]:\n",
        "            # Analyze the presentation using two different models\n",
        "            file_results1 = analyze_presentation(file.name, file_type, model_name1)\n",
        "            file_results2 = analyze_presentation(file.name, file_type, model_name2)\n",
        "\n",
        "            # Compare the outputs of the two models\n",
        "            similarity_score = compare_outputs(file_results1, file_results2)\n",
        "\n",
        "            combined_results[file.name] = {\n",
        "                \"Model 1 Output\": file_results1,\n",
        "                \"Model 2 Output\": file_results2,\n",
        "                \"Similarity Score (%)\": similarity_score\n",
        "            }\n",
        "        else:\n",
        "            combined_results[file.name] = {\"Error\": \"Unsupported file type. Please upload a .pptx or .pdf file\"}\n",
        "\n",
        "    return combined_results\n",
        "\n",
        "# Create Gradio interface for uploading files and selecting models\n",
        "iface = gr.Interface(\n",
        "    fn=process_presentations,\n",
        "    inputs=[\n",
        "        gr.Files(label=\"Upload your .pptx or .pdf files\"),\n",
        "        gr.Dropdown(\n",
        "            label=\"Select Primary Model\",\n",
        "            choices=[\n",
        "                \"llama3-groq-8b-8192-tool-use-preview\",\n",
        "                \"llama-3.2-3b-preview\",\n",
        "                \"llama-3.2-90b-vision-preview\"\n",
        "            ],\n",
        "            value=\"llama-3.2-90b-vision-preview\"\n",
        "        ),\n",
        "        gr.Dropdown(\n",
        "            label=\"Select Secondary Model\",\n",
        "            choices=[\n",
        "                \"llama3-groq-8b-8192-tool-use-preview\",\n",
        "                \"llama-3.2-3b-preview\",\n",
        "                \"llama-3.2-90b-vision-preview\"\n",
        "            ],\n",
        "            value=\"llama3-groq-8b-8192-tool-use-preview\"\n",
        "        )\n",
        "    ],\n",
        "    outputs=\"json\",\n",
        "    title=\"Business Term Sheet Analyzer with Model Comparison\",\n",
        "    description=\"Upload PowerPoint (.pptx) or PDF presentation files to extract business term sheet details and compare outputs between two models.\"\n",
        ")\n",
        "\n",
        "# Launch Gradio interface\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "TEcR5K7mf_y_",
        "outputId": "5048ca03-6e69-44cc-bdbd-ac9ab00428de"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-87382674b466>:35: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.9/migration/\n",
            "  @validator(\"important_dates\", \"investment_team_info\", pre=True, always=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://dea99ee215da3e865a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://dea99ee215da3e865a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lpmQ4woygkvv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}